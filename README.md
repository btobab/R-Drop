# AI-Studio-论文复现

## 项目描述
本repo主要复现苏大的论文R-Drop，并辅以deit_b_distilled_384

![](https://s3.bmp.ovh/imgs/2021/08/db570147c83e57f2.jpg)
## Abstract（摘要）
哈咯，大家好，我是生而为弟。本期我们主要介绍一下苏大的R-Drop。

众所周知，在训练深度神经网络时，正则化技术对于防止过拟合和提高深度模型的泛化能力是必不可少的。其中，dropout 技术是使用最广泛的一种，旨在通过在训练期间简单地从神经网络中删除一定比例的隐藏单元来避免协同适应并执行隐式集成。

好，现在我们已知dropout的牛逼，然而，在实作中，dropout往往是难以找到最优解。下面是我对于dropout难以找到最优解的理解，试想每一次前向计算，dropout都会随机丢弃一些隐藏单元，所有的丢弃的可能情况N可以用排列组合来计算，也正因如此，模型的参数空间也扩大了N倍，因此要找到最优解需要更多轮的训练以及更精细的调参。因此，苏大的大佬们一拍脑袋，想了个法子，既能充分利用dropout来应对过拟合，也能尽可能地缩小参数空间。

## Introduction（介绍）
首先有一个先验知识，大佬们认为，每一次前向计算都会随机丢掉一些隐藏单元，因此其实在每一轮计算时，模型的参数都是不同的。好，知道了这一点，下面我们继续。

平时我们在前向计算中都是只dropout一次，这回大佬们开了个脑洞，我们dropout两次试试。

具体来说，在每次 mini-batch 训练中，每个数据样本都经过两次前向传递，每次传递都由不同的子模型通过随机丢弃一些隐藏单元来处理。 R-Drop 通过最小化两个分布之间的双向KL散度，强制两个子模型输出的相同数据样本的两个分布彼此一致。也就是说，R-Drop正则化了模型训练中每个数据样本的两个采样子模型的输出。

下图形象地展示了该思想
![](https://ai-studio-static-online.cdn.bcebos.com/363f3be7960f4730afd9e242ae31c4ca261fcb9ea0fe4056b5ea9dced8c1caf3)


## Algorithm（算法）
![](https://ai-studio-static-online.cdn.bcebos.com/1df247052b844395a01c1869653d33c9e850b75338bc43f384970169a3ae7939)
![](https://ai-studio-static-online.cdn.bcebos.com/87fd2dc092874941aaadca3e4413556e8e5ecfe3280a403181f994198fee8890)


## Training Tips（训练技巧）
### A.准备工作
因为赛题要求使用ViT-B/16来复现，因此我选择了deit_b_distilled_384这个模型，并采用了PPIM仓库来调用预训练参数

R-Drop有一个很nice的地方就是对模型结构几乎不需要改动，因为大家一般都会在结构里加上dropout的，所以只需要对训练过程进行改动，想更优雅的还可以自定义一个损失函数。

对于超参数的选择我也是参考了原论文的train detail部分，KL散度损失的权重alpha设为5，dropout的概率设为0.3

### B.训练过程中遇到的问题以及解决策略
#### (1)精度66.7
刚开始训练时，在第一轮中，损失一直在上下浮动，无法稳定的下降，并且在第一轮训练结束后的测试中精度为0。当时实在有点怀疑人生，另外考虑到数据量有点大，一轮大概需要一小时，因此我决定先取1000
张图片作为训练集，100张作为验证集，看看是不是我的算法有问题。我花了点时间自己写了个Reader，然后就开始小规模训练了。结果训练了三轮测试精度都为0，emmm，当时我一度怀疑是预训练的参数有问题
因此我又尝试了其他的deit-B架构，但发现几轮下来精度还是0左右，因此我排除了预训练参数的问题。
然后我往上看了看超参数，学习率为1e-3，优化器为AdamW，emmm，是不是学习率有点大。之前看逼乎大佬说学习率要设大一点，不然学不到东西（大batch搭配大lr），当时的理解有些片面，只看到了大lr，
没看到大batch，我设置的batch只有24。但是当时我还没有反应过来，只是觉得学习率大了点，就随手调小了一个数量级，又开了。结果，我看了几章小说，一看精度，66.7，好家伙，逼乎真坑

#### (2)精度87.5
但是这里我还是在小样本上训练的，正准备上完整数据，突然灵机一动，虽然是小样本，但是模型已经适应了cifar100这个任务的一部分，那这个参数应该可以迁移到完整数据上。于是我读取了完整数据，导入了
在小样本上训练保存的参数，开启训练。一轮过后，精度87.5。啊这，我本来的预期是一轮过后精度能保持在66.7就很不错了，意外之喜^_^

#### (3)精度93.x(具体的忘了)
既然精度已经达到了66.7，说明模型已经大致收敛了，是时候上Momentum冲刺了。我设置了优化器Momentum，并加了L2 Decay 1e-2，同时设置了学习率的多项式衰减，因为Momentum的原因，将学习率调回了1e-3，开启训练，一轮过后，精度93.x，很好，很有精神= =

#### (4)精度100
既然精度已经到了93.x，那我就接着train呗，打游戏去了= = 打了一小时dnf，时间差不多，看了一下精度，emmm，还是93.x，这个就有点离谱了。我往上翻翻超参数，看看数据增广，都没啥好调的。
闲的无聊，看了看loss，诶，loss咋在上下蹦丫，从3.x到2.x到1.x，再从1.x到2.x到3.x，你特么搁着过山车呢。话虽如此，但当时我仍然没反应过来是什么原因，唔，喝了口水，上B站看看有啥好康的，
刚打开B站，我突然又有了灵感，这不就说明学习率太大了嘛，在flat minimum处反复横跳。彳亍，我将学习率调为3e-4，然后顺手把数据增广的水平翻转概率调大了点，就接着训练了。emmm，我也接着刷B站去了，一个小时后，再一看，精度100。我内心狂喜，但也不敢高兴的太早，毕竟只是验证集，但是既然已经到了100了，再训也没意思了，别整的过拟合了。然后我就提交了，泡了碗康师傅垫垫肚子，收工睡觉

## Conclusion（总结）
好啦，以上就是我这次复现的整个过程，实在是跌宕起伏，但是收获了很多。其实之前我也看了很多关于调参的博客，尤其是[山竹果姐姐的这篇博客](https://www.cnblogs.com/shona/p/12667950.html)，讲的特别详细。但是我只停留在知道的层次，并没有实战理解过。而本次复现让我对调参的知识加深了理解，我明白了为什么姐姐说学习率的设置是最最重要的，明白了为什么要进行学习率预热，明白了AdamW的学习率要尽可能小一点，明白了Momentum的学习率也不是一定要调大的，尤其是在调优冲刺的时候。

最后，感谢飞桨提供了这次有奖复现论文的机会，让我有足够的动力去调优，同时和我一起选R-Drop这篇论文的还有肖佬，这给了我极大的危机感，我不得不连肝两天，写到视野都模糊了才敢睡觉。

最后的最后，谢谢GT哥给我的技术帮助。
## 使用方式
### A
在AI Studio上[运行本项目](https://aistudio.baidu.com/aistudio/projectdetail/2249259)
### B
#### 使用前先从百度网盘上下载笔者训练好的最终参数，并将文件放置在与本项目的py文件相同的文件夹下。或自行配置路径
链接: https://pan.baidu.com/s/1OsuOqQVlxQ8oGO17HTWMDg 提取码: vug2

**开启训练**
```
python main.py
```
**开启测试**
```
python val.py
```
